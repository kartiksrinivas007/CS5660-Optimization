\section*{Theoretical Results}

\subsection*{Strong convexity of $\frac{1}{p} \| x \|_p^p$}
\begin{align*}
\Phi(x) &= \frac{1}{p} \| x \|_p^p \\
\nabla_i \Phi(x) &= \nabla_i \frac{\sum |x_j|^p}{p} \\
&= |x_i|^{p-1} \text{sgn}(x_i) \\
\nabla_{ij} \Phi(x) &= \begin{cases}(p-1)|x_i|^{p-2}\ \text{if i=j}\\ 0\ \text{o/w}\end{cases}
\end{align*}

For $p>1$, Hessian is positive semi-definite. If $x_i \ne 0\ \forall i$, then none of the eigen-values of the Hessian are 0. So $\Phi(x)$ is strongly convex.

\subsection*{Mirror Descent}
The function $\Phi(x) = \frac{1}{p} \| x \|_p^p$ is strongly convex with a parameter.

The map used in the problem takes the gradients to a different space namely the dual space of the system. 
The question is, what exactly is the norm that is being used on both of the sides.

The update equation is based on the fenchel conjugate of the function and is as follows
\begin{equation}
    \nabla w^*(y) = \text{argmax}_{x \in X} \langle x, y \rangle - w(x)
\end{equation}
Using this we get 
\begin{gather*}
    \nabla w^*(y) = \text{argmax}_{x \in X} \langle x, y \rangle - \frac{1}{p} \| x \|_p^p 
    \\
    y = \nabla_x \frac{\sum |x^*_j|^p}{p} \\
    \\
    y_j = |x^*_j|^{p-1} \text{sgn}(x_j) \\
    \\
    x_j^* = |y_j|^{1/(p-1)} \text{sgn}(y_j)
\end{gather*}

Therefore the update step is 

\begin{equation}
\label{eq:mirror_descent}
    x_{t+1} =  | \nabla w(x_t) - \eta \nabla f(x_t) |^{\frac{1}{p-1}} \text{sgn}(\nabla w(x_t) - \eta \nabla f(x_t))
\end{equation}

\subsection*{Proximal Mirror Descent}

For the proximal case the only additional update in the equation comes through the regularized norm  $\mu \| x \|_1$
The additional gradient is $\mu \text{ sgn }(x)$, which gets added to yield 

$$
y_j = \text{sgn}(x_j) ( \mu +  |x_j^*|^{p-1} )
$$

Hence we get 

$$
    x_j = \text{sgn}(y_j) \max(0, |y_j| - \mu)^{\frac{1}{p-1}}
$$

Note how the optimal solution to the problem can be seen component wise, since the function $\frac{1}{p} \| x \|^p$ can be broken component-wise into several parts.
The argmax would be a cartesian product of each component wise optimal $x_j^*$. The stationary point will exist only when $|y_j| \le \mu$


\begin{align*}
    f'(x_j) &= \nabla_{x_j} (x_jy_j  - \frac{1}{p} |x_j|^p - |x_j|) \\\
    &= y_j - |x_j|^{p-1} - \text{sgn}(x_j) \\
    &= \begin{cases} 
        < 0 & x_j\leq 0, |y_j| < \mu \\
        > 0  & x_j \geq 0, |y_j| < \mu  \\
     \end{cases}\\
    &= 0 \text{ if } |y_j| \ge \mu \text{ and } y_j = \text{sgn}(x_j) ( \mu + |x_j|^{p-1})\\
\end{align*}

In our case the proxy for $\mu$ is the multiplication of the step size $\eta$ and the regularization weight $\lambda$, i.e $\mu = \lambda \eta$

\begin{equation}
    \label{eq: psmd}
    x_{t + 1} = \text{sgn}(\nabla w(x_t) - \eta \nabla f(x_t)) \max(0, |\nabla w(x_t) - \eta \nabla f(x_t)| - \lambda \eta)^{\frac{1}{p-1}}
\end{equation}

where $\nabla w(x) = |x|^{p-1} \text{sgn}(x)$

\subsection*{Accelerated Proximal Mirror Descent}

We can accelerate the Proximal Mirror Descent Algorithm using the Nesterov Trick. Here the weight update equation is
\begin{equation}
w_{t+1} = w_t + \gamma_t \Delta w_{t-1} -\eta_t \nabla \mathcal{L}(w_t+\gamma_t\Delta w_{t-1})
\end{equation}
Here, $\Delta w_{t-1} = w_t-w_{t-1}$ and $\gamma_t$ is the momentum parameter. In Dual space this equation is 
\begin{equation}
\nabla \psi(w_{t+1}) = \nabla \psi(w_t) + \gamma_t\Delta z_{t-1} - \eta_t \nabla\mathcal{L}(w_t+\gamma_t \Delta w_{t-1})
\end{equation} 
Here, $\Delta z_{t-1} = \nabla \psi(w_t)-\nabla \psi(w_{t-1})$ and $\nabla \psi(w_t)$ are the dual variables.

Using previous equations for proximal mirror descent, we can find the new update equation as 
\begin{align*}
y_t &= \nabla w(x_t) + \gamma (\nabla w(x_t)-\nabla w(x_{t-1})) - \eta \nabla f(x_t+\gamma (x_t-x_{t-1}))\\
x_{t+1} &= \text{sgn}(y_t) \max(0, |y_t| - \lambda \eta)^{\frac{1}{p-1}}
\end{align*}
where $\nabla w(x) = |x|^{p-1} \text{sgn}(x)$.

We observed that pytorch \href{https://pytorch.org/docs/stable/generated/torch.optim.SGD.html}{implementation} approximates the Nesterov momentum equations as 
\begin{align*}
v_{t+1} &= \gamma v_t + \nabla f(x_t)\\
x_{t+1} &= x_t - \eta v_{t+1}
\end{align*}

However, we used the original Nesterov equations given as
\begin{align*}
v_{t+1} &= \gamma v_t - \eta \nabla f(x_t+\gamma v_t)\\
x_{t+1} &= x_t + v_{t+1}
\end{align*}

In dual space, these equations become
\begin{align*}
z_{t+1} &= \gamma z_t - \eta \nabla f(x_t+\gamma (x_t-x_{t-1}))\\
y_{t+!} &= y_t + z_{t+1}\\
x_{t+1} &= \text{sgn}(y_{t+1}) \max(0, |y_{t+1}| - \lambda \eta)^{\frac{1}{p-1}}
\end{align*}

Since we want to compute the gradient at $x_t+\gamma (x_t-x_{t-1})$, we update the weights to $x_t+\gamma (x_t-x_{t-1})$ in practice and only use the actual update at the last update step. 

At the first update, we ensure initialisation is such that the first step is same as Proximal Mirror Descent update step (momentum is 0).